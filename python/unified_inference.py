#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Unified Inference - mBART + MT5 Consensus Model
KiÅŸiselleÅŸtirilmiÅŸ feedback sistemi ile
"""

import sys
import io
import json
import torch
import logging
import random
from pathlib import Path
from transformers import MBartForConditionalGeneration, MBartTokenizer
from transformers import MT5ForConditionalGeneration, MT5Tokenizer

# Windows iÃ§in encoding ayarÄ±
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')

# Logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class UnifiedInference:
    def __init__(self):
        """Initialize unified inference system"""
        logger.info("ğŸš€ Unified Inference baÅŸlatÄ±lÄ±yor...")
        
        # Device
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        logger.info(f"ğŸ–¥ï¸  Cihaz: {self.device}")
        
        # Model paths - Hugging Face'ten yÃ¼kle
        self.mbart_path = "Ozget/MetaMind_Nlp_MBART_2"
        self.mt5_path = "Ozget/MetaMind_Nlp_MT5_2"
        
        logger.info(f"ğŸ“‚ mBART model: {self.mbart_path}")
        logger.info(f"ğŸ“‚ MT5 model: {self.mt5_path}")
        
        # Models
        self.mbart_model = None
        self.mbart_tokenizer = None
        self.mt5_model = None
        self.mt5_tokenizer = None
        
        # Label mapping
        self.label_map = ['YanlÄ±ÅŸ', 'KÄ±smen DoÄŸru', 'Ã‡ok Benzer', 'Tam DoÄŸru']
        
    def load_models(self):
        """TÃ¼m modelleri yÃ¼kle"""
        try:
            logger.info("ğŸ“¦ Modeller Hugging Face'ten yÃ¼kleniyor...")
            
            # mBART yÃ¼kle
            try:
                logger.info(f"ğŸ“¥ mBART yÃ¼kleniyor: {self.mbart_path}")
                self.mbart_tokenizer = MBartTokenizer.from_pretrained(self.mbart_path)
                self.mbart_model = MBartForConditionalGeneration.from_pretrained(self.mbart_path)
                self.mbart_model.to(self.device)
                self.mbart_model.eval()
                logger.info("âœ… mBART hazÄ±r!")
            except Exception as e:
                logger.warning(f"âš ï¸ mBART yÃ¼klenemedi: {e}")
            
            # MT5 yÃ¼kle
            try:
                logger.info(f"ğŸ“¥ MT5 yÃ¼kleniyor: {self.mt5_path}")
                self.mt5_tokenizer = MT5Tokenizer.from_pretrained(self.mt5_path)
                self.mt5_model = MT5ForConditionalGeneration.from_pretrained(self.mt5_path)
                self.mt5_model.to(self.device)
                self.mt5_model.eval()
                logger.info("âœ… MT5 hazÄ±r!")
            except Exception as e:
                logger.warning(f"âš ï¸ MT5 yÃ¼klenemedi: {e}")
            
            # En az bir model yÃ¼klenmeli
            if not self.mbart_model and not self.mt5_model:
                raise Exception("HiÃ§bir model yÃ¼klenemedi!")
            
            logger.info("ğŸ‰ TÃ¼m modeller yÃ¼klendi!")
            
            # Modellerin hazÄ±r olduÄŸunu bildir
            print(json.dumps({
                "status": "models_loaded",
                "success": True,
                "mbart_loaded": self.mbart_model is not None,
                "mt5_loaded": self.mt5_model is not None
            }, ensure_ascii=False), flush=True)
            
        except Exception as e:
            logger.error(f"âŒ Model yÃ¼kleme hatasÄ±: {e}")
            print(json.dumps({
                "status": "models_failed",
                "success": False,
                "error": str(e)
            }, ensure_ascii=False), flush=True)
            raise
    
    def predict_with_mbart(self, question: str, student_answer: str, correct_answer: str) -> dict:
        """mBART ile tahmin - Her seferinde UNIQUE Ã§Ä±ktÄ±"""
        if not self.mbart_model:
            return None
        
        try:
            # mBART iÃ§in farklÄ± prompt varyasyonlarÄ±
            prompt_variants = [
                f"Soru: {question} Ã–ÄŸrenci CevabÄ±: {student_answer} Hedef Cevap: {correct_answer}",
                f"Quiz: {question}\nCevap: {student_answer}\nDoÄŸru: {correct_answer}",
                f"DeÄŸerlendirme: '{question}' sorusuna '{student_answer}' cevabÄ±. Beklenen: '{correct_answer}'",
                f"Analiz: Soru='{question}', Verilen='{student_answer}', Hedef='{correct_answer}'",
            ]
            input_text = random.choice(prompt_variants)
            
            inputs = self.mbart_tokenizer(
                input_text,
                max_length=256,
                padding='max_length',
                truncation=True,
                return_tensors='pt'
            ).to(self.device)
            
            # UNIQUE generation parametreleri
            with torch.no_grad():
                outputs = self.mbart_model.generate(
                    input_ids=inputs['input_ids'],
                    attention_mask=inputs['attention_mask'],
                    max_length=256,
                    do_sample=True,  # Sampling aktif - her seferinde farklÄ±
                    temperature=0.8,  # YaratÄ±cÄ±lÄ±k
                    top_p=0.92,  # Nucleus sampling
                    top_k=50,  # Top-K sampling
                    repetition_penalty=1.2,  # TekrarlarÄ± azalt
                    no_repeat_ngram_size=3  # 3-gram tekrarÄ± engelle
                )
            
            prediction = self.mbart_tokenizer.decode(outputs[0], skip_special_tokens=True)
            label, feedback = self.parse_output(prediction)
            label_code = self.get_label_code(label)
            
            return {
                "model": "mBART",
                "raw_output": prediction,
                "label": label,
                "label_code": label_code,
                "feedback": feedback,
                "confidence": 85
            }
        except Exception as e:
            logger.error(f"mBART prediction error: {e}")
            return None
    
    def predict_with_mt5(self, question: str, student_answer: str, correct_answer: str) -> dict:
        """MT5 ile tahmin - Her seferinde UNIQUE Ã§Ä±ktÄ±"""
        if not self.mt5_model:
            return None
        
        try:
            # MT5 iÃ§in farklÄ± prompt varyasyonlarÄ±
            prompt_variants = [
                f"Soru: {question}\nCevap: {student_answer}\nDoÄŸru: {correct_answer}\nPuanla ve yorum yap:",
                f"Ã–ÄŸrenci DeÄŸerlendirme:\nSoru: {question}\nVerilen Cevap: {student_answer}\nBeklenen: {correct_answer}",
                f"Quiz Analizi:\n'{question}' sorusuna '{student_answer}' cevabÄ± verildi. DoÄŸrusu '{correct_answer}'. DeÄŸerlendir:",
                f"Akademik DeÄŸerlendirme:\nSoru: {question}\nÃ–ÄŸrencinin Yorumu: {student_answer}\nStandart Cevap: {correct_answer}\nFeedback:",
            ]
            input_text = random.choice(prompt_variants)
            
            inputs = self.mt5_tokenizer(
                input_text,
                max_length=256,
                padding='max_length',
                truncation=True,
                return_tensors='pt'
            ).to(self.device)
            
            # UNIQUE generation parametreleri - MT5 iÃ§in biraz daha yaratÄ±cÄ±
            with torch.no_grad():
                outputs = self.mt5_model.generate(
                    input_ids=inputs['input_ids'],
                    attention_mask=inputs['attention_mask'],
                    max_length=256,
                    do_sample=True,  # Sampling aktif
                    temperature=0.85,  # MT5 iÃ§in biraz daha yaratÄ±cÄ±
                    top_p=0.9,  # Nucleus sampling
                    top_k=40,  # Top-K sampling
                    repetition_penalty=1.3,  # TekrarlarÄ± daha fazla azalt
                    no_repeat_ngram_size=3  # 3-gram tekrarÄ± engelle
                )
            
            prediction = self.mt5_tokenizer.decode(outputs[0], skip_special_tokens=True)
            label, feedback = self.parse_output(prediction)
            label_code = self.get_label_code(label)
            
            return {
                "model": "MT5",
                "raw_output": prediction,
                "label": label,
                "label_code": label_code,
                "feedback": feedback,
                "confidence": 82
            }
        except Exception as e:
            logger.error(f"MT5 prediction error: {e}")
            return None
    
    def get_label_code(self, label: str) -> int:
        """Label'Ä± sayÄ±sal koda Ã§evir"""
        label_codes = {
            'YanlÄ±ÅŸ': 0,
            'KÄ±smen DoÄŸru': 1,
            'Ã‡ok Benzer': 2,
            'Tam DoÄŸru': 3
        }
        return label_codes.get(label, 0)
    
    def parse_output(self, output: str) -> tuple:
        """Model output'unu parse et"""
        try:
            label = "YanlÄ±ÅŸ"
            feedback = "CevabÄ±nÄ±zÄ± gÃ¶zden geÃ§irin."
            
            if "Etiket:" in output:
                parts = output.split("Geri Bildirim:")
                
                label_part = parts[0].replace("Etiket:", "").strip()
                for lbl in self.label_map:
                    if lbl.lower() in label_part.lower():
                        label = lbl
                        break
                
                if len(parts) > 1:
                    feedback = parts[1].strip()
            else:
                for lbl in self.label_map:
                    if lbl.lower() in output.lower():
                        label = lbl
                        break
                feedback = output.strip()
            
            return label, feedback
            
        except Exception as e:
            logger.error(f"Parse error: {e}")
            return "YanlÄ±ÅŸ", output
    
    def create_personalized_feedback(self, label: str, student_answer: str, correct_answer: str, 
                                     student_confidence: int, topic: str, question: str) -> str:
        """Konuyu ve gÃ¼ven skorunu iÃ§eren Ã¶zel feedback oluÅŸtur"""
        
        # GÃ¼ven skoru analizi
        conf_text = ""
        if student_confidence is not None:
            if student_confidence >= 80:
                conf_text = f"CevabÄ±nÄ±za %{student_confidence} gÃ¼venle yaklaÅŸtÄ±nÄ±z. "
            elif student_confidence >= 60:
                conf_text = f"CevabÄ±nÄ±za orta dÃ¼zeyde (%{student_confidence}) gÃ¼vendiniz. "
            elif student_confidence >= 40:
                conf_text = f"CevabÄ±nÄ±za %{student_confidence} gÃ¼venle yaklaÅŸtÄ±nÄ±z, bu da tereddÃ¼tleriniz olduÄŸunu gÃ¶steriyor. "
            else:
                conf_text = f"CevabÄ±nÄ±za Ã§ok az (%{student_confidence}) gÃ¼vendiniz. "
        
        # Konu bilgisi
        topic_text = f"{topic} konusunda " if topic else ""
        
        # Label'a gÃ¶re Ã¶zel mesajlar
        if label == "Tam DoÄŸru":
            messages = [
                f"ğŸ¯ MÃ¼kemmel! {topic_text}'{correct_answer}' cevabÄ±nÄ± tam doÄŸru verdiniz. {conf_text}Bilginiz ve farkÄ±ndalÄ±ÄŸÄ±nÄ±z harika!",
                f"âœ¨ Harika! {topic_text}cevabÄ±nÄ±z tamamen doÄŸru. {conf_text}Bu konuda kendinize gÃ¼venebilirsiniz!",
                f"ğŸŒŸ SÃ¼per! {topic_text}'{student_answer}' cevabÄ± kesinlikle doÄŸru. {conf_text}Tebrikler!",
                f"ğŸ’¯ MÃ¼kemmel performans! {topic_text}cevabÄ±nÄ±z birebir doÄŸru. {conf_text}BÃ¶yle devam edin!"
            ]
        elif label == "Ã‡ok Benzer":
            messages = [
                f"ğŸ‘ Ä°yi! {topic_text}cevabÄ±nÄ±z Ã§ok yakÄ±n. {conf_text}DoÄŸru yoldasÄ±nÄ±z, kÃ¼Ã§Ã¼k detaylara dikkat ederseniz mÃ¼kemmel olacak!",
                f"ğŸ“Œ GÃ¼zel! {topic_text}cevabÄ±nÄ±z doÄŸruya Ã§ok benziyor. {conf_text}Biraz daha kesinlik kazanÄ±rsanÄ±z tam puan!",
                f"âœ… YaklaÅŸtÄ±nÄ±z! {topic_text}'{student_answer}' cevabÄ±nÄ±z doÄŸruya oldukÃ§a yakÄ±n. {conf_text}Ä°yi gidiyorsunuz!",
                f"ğŸ¯ Neredeyse! {topic_text}cevabÄ±nÄ±z Ã§ok iyi. {conf_text}KÃ¼Ã§Ã¼k bir ince ayar ile mÃ¼kemmel olacak!"
            ]
        elif label == "KÄ±smen DoÄŸru":
            messages = [
                f"ğŸ“š {topic_text}cevabÄ±nÄ±z kÄ±smen doÄŸru. {conf_text}Konuyu biraz daha pekiÅŸtirmenizi Ã¶neririm. DoÄŸru cevap: '{correct_answer}'",
                f"ğŸ’ª {topic_text}yoldasÄ±nÄ±z ama tam deÄŸil. {conf_text}'{correct_answer}' ÅŸeklinde olmasÄ± gerekiyordu. Biraz daha Ã§alÄ±ÅŸma ile baÅŸarÄ±rsÄ±nÄ±z!",
                f"ğŸ”„ {topic_text}cevabÄ±nÄ±zda doÄŸrular var. {conf_text}Ama tam deÄŸil. DoÄŸrusu '{correct_answer}'. Bu konuyu gÃ¶zden geÃ§irin!",
                f"ğŸ“– {topic_text}yaklaÅŸtÄ±nÄ±z. {conf_text}Ancak '{correct_answer}' olmasÄ± gerekiyordu. Bu konuda biraz daha pratik yapÄ±n!"
            ]
        else:  # YanlÄ±ÅŸ
            messages = [
                f"âŒ {topic_text}cevabÄ±nÄ±z yanlÄ±ÅŸ. {conf_text}DoÄŸru cevap '{correct_answer}'. Bu konuyu tekrar Ã§alÄ±ÅŸmanÄ±zÄ± Ã¶neririm.",
                f"ğŸ”´ {topic_text}maalesef doÄŸru deÄŸil. {conf_text}'{correct_answer}' olmasÄ± gerekiyordu. Bu konuya daha fazla zaman ayÄ±rÄ±n!",
                f"ğŸ“• {topic_text}'{student_answer}' cevabÄ± yanlÄ±ÅŸ. {conf_text}DoÄŸrusu '{correct_answer}'. Bu konuyu gÃ¶zden geÃ§irin!",
                f"âš ï¸ {topic_text}cevabÄ±nÄ±z hatalÄ±. {conf_text}DoÄŸru cevap '{correct_answer}'. Konu tekrarÄ± yapmanÄ±z faydalÄ± olacak!"
            ]
        
        return random.choice(messages)
    
    def get_consensus_result(self, mbart_result: dict, mt5_result: dict) -> dict:
        """Ä°ki modelin sonucunu birleÅŸtir"""
        
        label_scores = {
            'Tam DoÄŸru': 3,
            'Ã‡ok Benzer': 2,
            'KÄ±smen DoÄŸru': 1,
            'YanlÄ±ÅŸ': 0
        }
        
        # Her iki model de Ã§alÄ±ÅŸmadÄ±ysa
        if not mbart_result and not mt5_result:
            return {
                "final_label": "YanlÄ±ÅŸ",
                "final_feedback": "Model analizi baÅŸarÄ±sÄ±z oldu.",
                "confidence": 0
            }
        
        # Sadece bir model Ã§alÄ±ÅŸtÄ±ysa
        if not mbart_result:
            return {
                "final_label": mt5_result['label'],
                "final_feedback": mt5_result['feedback'],
                "confidence": mt5_result['confidence'] / 100
            }
        if not mt5_result:
            return {
                "final_label": mbart_result['label'],
                "final_feedback": mbart_result['feedback'],
                "confidence": mbart_result['confidence'] / 100
            }
        
        # Her iki model de Ã§alÄ±ÅŸÄ±yorsa - konsensÃ¼s
        mbart_score = label_scores.get(mbart_result['label'], 0)
        mt5_score = label_scores.get(mt5_result['label'], 0)
        
        # Ortalama skor
        avg_score = (mbart_score + mt5_score) / 2
        
        # En yakÄ±n label'Ä± bul
        final_label = "YanlÄ±ÅŸ"
        for label, score in label_scores.items():
            if abs(score - avg_score) < 0.6:
                final_label = label
                break
        
        # Feedback - daha detaylÄ± olanÄ± seÃ§
        final_feedback = mbart_result['feedback'] if len(mbart_result['feedback']) > len(mt5_result['feedback']) else mt5_result['feedback']
        
        # Confidence - ortalama
        confidence = (mbart_result['confidence'] + mt5_result['confidence']) / 200  # Normalize to 0-1
        
        return {
            "final_label": final_label,
            "final_feedback": final_feedback,
            "confidence": confidence
        }
    
    def analyze(self, question: str, student_answer: str, correct_answer: str, student_confidence: int = None, topic: str = None) -> dict:
        """Ana analiz fonksiyonu"""
        try:
            logger.info(f"ğŸ“ Analiz baÅŸlÄ±yor...")
            logger.info(f"Soru: {question[:50]}...")
            logger.info(f"Ã–ÄŸrenci: {student_answer[:50]}...")
            
            mbart_result = self.predict_with_mbart(question, student_answer, correct_answer)
            mt5_result = self.predict_with_mt5(question, student_answer, correct_answer)
            
            consensus = self.get_consensus_result(mbart_result, mt5_result)
            
            # Ã–zel feedback - konuyu ve gÃ¼ven skorunu iÃ§eren
            personalized_feedback = self.create_personalized_feedback(
                consensus['final_label'],
                student_answer,
                correct_answer,
                student_confidence,
                topic,
                question
            )
            
            agent_result = {
                "chosen_model": "mBART + MT5 Consensus",
                "label": consensus['final_label'],
                "feedback": personalized_feedback,
                "confidence": consensus['confidence'],
                "reasoning": f"mBART ve MT5 modellerinin ortak kararÄ±. Ä°ki model birlikte '{consensus['final_label']}' sonucuna vardÄ±."
            }
            
            result = {
                "success": True,
                "models": {
                    "mbart": mbart_result if mbart_result else {"label": "Analiz baÅŸarÄ±sÄ±z", "label_code": 0, "feedback": "Model yÃ¼klenmedi", "confidence": 0},
                    "mt5": mt5_result if mt5_result else {"label": "Analiz baÅŸarÄ±sÄ±z", "label_code": 0, "feedback": "Model yÃ¼klenmedi", "confidence": 0},
                    "agent": agent_result
                },
                "consensus": consensus,
                "label": consensus['final_label'],
                "feedback": personalized_feedback,
                "confidence": consensus['confidence']
            }
            
            logger.info(f"âœ… Analiz tamamlandÄ±: {consensus['final_label']}")
            return result
            
        except Exception as e:
            logger.error(f"âŒ Analiz hatasÄ±: {e}")
            return {
                "success": False,
                "error": str(e),
                "label": "YanlÄ±ÅŸ",
                "feedback": "Analiz sÄ±rasÄ±nda hata oluÅŸtu.",
                "models": {
                    "mbart": {"label": "Hata", "label_code": 0, "feedback": str(e), "confidence": 0},
                    "mt5": {"label": "Hata", "label_code": 0, "feedback": str(e), "confidence": 0},
                    "agent": {"chosen_model": "None", "label": "Hata", "feedback": str(e), "confidence": 0, "reasoning": "Hata oluÅŸtu"}
                }
            }

def main():
    """Ana fonksiyon - stdin'den JSON al, stdout'a JSON yaz"""
    
    inferencer = UnifiedInference()
    inferencer.load_models()
    
    logger.info("ğŸ§ Ä°stekler bekleniyor (stdin)...")
    
    for line in sys.stdin:
        try:
            line = line.strip()
            if not line:
                continue
                
            data = json.loads(line)
            question = data.get('question', '')
            student_answer = data.get('student_answer', '')
            correct_answer = data.get('correct_answer', '')
            student_confidence = data.get('student_confidence', None)  # Yeni
            topic = data.get('topic', None)  # Yeni
            
            result = inferencer.analyze(question, student_answer, correct_answer, student_confidence, topic)
            
            print(json.dumps(result, ensure_ascii=False), flush=True)
                
        except json.JSONDecodeError as e:
            logger.error(f"JSON parse hatasÄ±: {e}")
            print(json.dumps({
                "success": False,
                "error": f"Invalid JSON: {e}"
            }), flush=True)
        except Exception as e:
            logger.error(f"Ä°ÅŸlem hatasÄ±: {e}")
            print(json.dumps({
                "success": False,
                "error": str(e)
            }), flush=True)

if __name__ == "__main__":
    main()

