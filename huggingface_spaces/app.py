"""
MetaMind NLP - Ã–ÄŸrenci Cevap Analizi
Gradio Interface
"""

import gradio as gr
import torch
import logging
import json
from transformers import MBartForConditionalGeneration, MBartTokenizer
from transformers import MT5ForConditionalGeneration, MT5Tokenizer
import random

# Logging ayarlarÄ±
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Global deÄŸiÅŸkenler
mbart_model = None
mbart_tokenizer = None
mt5_model = None
mt5_tokenizer = None
device = None
label_map = ['YanlÄ±ÅŸ', 'KÄ±smen DoÄŸru', 'Ã‡ok Benzer', 'Tam DoÄŸru']

# Model hassasiyet ayarlarÄ±
SIMILARITY_THRESHOLD = 0.7  # Benzerlik eÅŸiÄŸi (0.0-1.0)
STRICT_MODE = True  # KatÄ± mod - daha az tolerans
CONFIDENCE_THRESHOLD = 0.6  # GÃ¼ven eÅŸiÄŸi

def load_models():
    """Modelleri baÅŸlangÄ±Ã§ta yÃ¼kle"""
    global mbart_model, mbart_tokenizer, mt5_model, mt5_tokenizer, device
    
    logger.info("ğŸš€ MetaMind NLP baÅŸlatÄ±lÄ±yor...")
    
    # Device belirleme
    device = "cuda" if torch.cuda.is_available() else "cpu"
    logger.info(f"ğŸ–¥ï¸  Cihaz: {device}")
    
    # mBART modeli
    try:
        logger.info("ğŸ“¥ mBART modeli yÃ¼kleniyor...")
        from transformers import MBartConfig
        
        # Ã–nce config'i yÃ¼kle ve dÃ¼zelt
        config = MBartConfig.from_pretrained("Ozget/MetaMind_Nlp_MBART_2")
        config.early_stopping = True
        
        # Tokenizer'Ä± yÃ¼kle
        mbart_tokenizer = MBartTokenizer.from_pretrained("Ozget/MetaMind_Nlp_MBART_2")
        
        # Modeli dÃ¼zeltilmiÅŸ config ile yÃ¼kle
        mbart_model = MBartForConditionalGeneration.from_pretrained(
            "Ozget/MetaMind_Nlp_MBART_2",
            config=config,
            ignore_mismatched_sizes=True
        )
        mbart_model.to(device)
        mbart_model.eval()
        logger.info("âœ… mBART hazÄ±r!")
    except Exception as e:
        logger.error(f"âŒ mBART yÃ¼klenemedi: {e}")
        mbart_model = None
        mbart_tokenizer = None
    
    # MT5 modeli
    try:
        logger.info("ğŸ“¥ MT5 modeli yÃ¼kleniyor...")
        from transformers import MT5Config
        
        # Ã–nce config'i yÃ¼kle ve dÃ¼zelt
        config = MT5Config.from_pretrained("Ozget/MetaMind_Nlp_MT5_2")
        config.early_stopping = True
        
        # Tokenizer'Ä± yÃ¼kle
        mt5_tokenizer = MT5Tokenizer.from_pretrained("Ozget/MetaMind_Nlp_MT5_2")
        
        # Modeli dÃ¼zeltilmiÅŸ config ile yÃ¼kle
        mt5_model = MT5ForConditionalGeneration.from_pretrained(
            "Ozget/MetaMind_Nlp_MT5_2",
            config=config,
            ignore_mismatched_sizes=True
        )
        mt5_model.to(device)
        mt5_model.eval()
        logger.info("âœ… MT5 hazÄ±r!")
    except Exception as e:
        logger.error(f"âŒ MT5 yÃ¼klenemedi: {e}")
        mt5_model = None
        mt5_tokenizer = None
    
    if not mbart_model and not mt5_model:
        raise Exception("âŒ HiÃ§bir model yÃ¼klenemedi!")
    
    logger.info("ğŸ‰ TÃ¼m modeller hazÄ±r!")

def get_label_code(label: str) -> int:
    """Label'Ä± sayÄ±sal koda Ã§evir"""
    label_codes = {'YanlÄ±ÅŸ': 0, 'KÄ±smen DoÄŸru': 1, 'Ã‡ok Benzer': 2, 'Tam DoÄŸru': 3}
    return label_codes.get(label, 0)

def parse_output(output: str) -> tuple:
    """Model Ã§Ä±ktÄ±sÄ±nÄ± parse et - KatÄ± mod"""
    try:
        label = "YanlÄ±ÅŸ"  # VarsayÄ±lan olarak yanlÄ±ÅŸ
        feedback = "CevabÄ±nÄ±zÄ± gÃ¶zden geÃ§irin."
        
        # Ã‡Ä±ktÄ±yÄ± temizle ve kÃ¼Ã§Ã¼k harfe Ã§evir
        clean_output = output.lower().strip()
        
        # KatÄ± mod: Sadece aÃ§Ä±k etiketler kabul edilir
        if STRICT_MODE:
            # Tam eÅŸleÅŸme aranÄ±r
            if "tam doÄŸru" in clean_output or "tamdogru" in clean_output:
                label = "Tam DoÄŸru"
            elif "Ã§ok benzer" in clean_output or "cokbenzer" in clean_output:
                label = "Ã‡ok Benzer"
            elif "kÄ±smen doÄŸru" in clean_output or "kismendogru" in clean_output:
                label = "KÄ±smen DoÄŸru"
            else:
                label = "YanlÄ±ÅŸ"  # Belirsiz durumlarda yanlÄ±ÅŸ
        else:
            # Normal mod: Daha esnek
            if "Etiket:" in output:
                parts = output.split("Geri Bildirim:")
                label_part = parts[0].replace("Etiket:", "").strip()
                for lbl in label_map:
                    if lbl.lower() in label_part.lower():
                        label = lbl
                        break
                if len(parts) > 1:
                    feedback = parts[1].strip()
            else:
                for lbl in label_map:
                    if lbl.lower() in output.lower():
                        label = lbl
                        break
                feedback = output.strip()
        
        # GÃ¼ven kontrolÃ¼ - dÃ¼ÅŸÃ¼k gÃ¼ven varsa yanlÄ±ÅŸ yap
        if "gÃ¼ven" in clean_output or "confidence" in clean_output:
            if any(word in clean_output for word in ["dÃ¼ÅŸÃ¼k", "dusuk", "az", "low"]):
                label = "YanlÄ±ÅŸ"
        
        return label, feedback
    except Exception as e:
        logger.error(f"Parse hatasÄ±: {e}")
        return "YanlÄ±ÅŸ", output

def predict_mbart(question: str, student_answer: str, correct_answer: str) -> dict:
    """mBART ile tahmin"""
    if not mbart_model:
        return None
    
    try:
        prompts = [
            f"Soru: {question} Ã–ÄŸrenci: {student_answer} DoÄŸru: {correct_answer}",
            f"Quiz: {question}\nCevap: {student_answer}\nHedef: {correct_answer}",
        ]
        input_text = random.choice(prompts)
        
        inputs = mbart_tokenizer(
            input_text, max_length=256, padding='max_length', 
            truncation=True, return_tensors='pt'
        ).to(device)
        
        with torch.no_grad():
            # KatÄ± mod parametreleri
            if STRICT_MODE:
                outputs = mbart_model.generate(
                    input_ids=inputs['input_ids'],
                    attention_mask=inputs['attention_mask'],
                    max_length=128,  # Daha kÄ±sa Ã§Ä±ktÄ±
                    do_sample=False,  # Deterministik
                    num_beams=4,  # Beam search
                    early_stopping=True,
                    repetition_penalty=1.5,  # Daha yÃ¼ksek tekrar cezasÄ±
                    no_repeat_ngram_size=2
                )
            else:
                outputs = mbart_model.generate(
                    input_ids=inputs['input_ids'],
                    attention_mask=inputs['attention_mask'],
                    max_length=256, 
                    do_sample=True, 
                    temperature=0.8,
                    top_p=0.92, 
                    top_k=50, 
                    repetition_penalty=1.2,
                    no_repeat_ngram_size=3,
                    early_stopping=True
                )
        
        prediction = mbart_tokenizer.decode(outputs[0], skip_special_tokens=True)
        label, feedback = parse_output(prediction)
        
        return {"model": "mBART", "label": label, "label_code": get_label_code(label), 
                "feedback": feedback, "confidence": 85}
    except Exception as e:
        logger.error(f"mBART hatasÄ±: {e}")
        return None

def predict_mt5(question: str, student_answer: str, correct_answer: str) -> dict:
    """MT5 ile tahmin"""
    if not mt5_model:
        return None
    
    try:
        prompts = [
            f"Soru: {question}\nCevap: {student_answer}\nDoÄŸru: {correct_answer}\nDeÄŸerlendir:",
            f"Quiz Analizi: '{question}' - '{student_answer}' vs '{correct_answer}'",
        ]
        input_text = random.choice(prompts)
        
        inputs = mt5_tokenizer(
            input_text, max_length=256, padding='max_length',
            truncation=True, return_tensors='pt'
        ).to(device)
        
        with torch.no_grad():
            # KatÄ± mod parametreleri
            if STRICT_MODE:
                outputs = mt5_model.generate(
                    input_ids=inputs['input_ids'],
                    attention_mask=inputs['attention_mask'],
                    max_length=128,  # Daha kÄ±sa Ã§Ä±ktÄ±
                    do_sample=False,  # Deterministik
                    num_beams=4,  # Beam search
                    early_stopping=True,
                    repetition_penalty=1.6,  # Daha yÃ¼ksek tekrar cezasÄ±
                    no_repeat_ngram_size=2
                )
            else:
                outputs = mt5_model.generate(
                    input_ids=inputs['input_ids'],
                    attention_mask=inputs['attention_mask'],
                    max_length=256, 
                    do_sample=True, 
                    temperature=0.85,
                    top_p=0.9, 
                    top_k=40, 
                    repetition_penalty=1.3,
                    no_repeat_ngram_size=3,
                    early_stopping=True
                )
        
        prediction = mt5_tokenizer.decode(outputs[0], skip_special_tokens=True)
        label, feedback = parse_output(prediction)
        
        return {"model": "MT5", "label": label, "label_code": get_label_code(label),
                "feedback": feedback, "confidence": 82}
    except Exception as e:
        logger.error(f"MT5 hatasÄ±: {e}")
        return None

def create_feedback(label: str, student_answer: str, correct_answer: str, 
                    confidence: int, topic: str) -> str:
    """KiÅŸiselleÅŸtirilmiÅŸ feedback oluÅŸtur"""
    conf_text = ""
    if confidence and confidence > 0:
        if confidence >= 80:
            conf_text = f"CevabÄ±nÄ±za %{confidence} gÃ¼vendiniz. "
        elif confidence >= 60:
            conf_text = f"Orta dÃ¼zeyde (%{confidence}) gÃ¼vendiniz. "
        else:
            conf_text = f"Az gÃ¼venle (%{confidence}) yaklaÅŸtÄ±nÄ±z. "
    
    topic_text = f"{topic} konusunda " if topic else ""
    
    messages = {
        "Tam DoÄŸru": [
            f"ğŸ¯ MÃ¼kemmel! {topic_text}'{correct_answer}' tam doÄŸru! {conf_text}",
            f"âœ¨ Harika! {topic_text}CevabÄ±nÄ±z kesinlikle doÄŸru. {conf_text}",
        ],
        "Ã‡ok Benzer": [
            f"ğŸ‘ Ä°yi! {topic_text}CevabÄ±nÄ±z Ã§ok yakÄ±n. {conf_text}",
            f"ğŸ“Œ GÃ¼zel! {topic_text}'{student_answer}' neredeyse doÄŸru. {conf_text}",
        ],
        "KÄ±smen DoÄŸru": [
            f"ğŸ“š {topic_text}KÄ±smen doÄŸru. {conf_text}DoÄŸrusu: '{correct_answer}'",
            f"ğŸ’ª {topic_text}YoldasÄ±nÄ±z! {conf_text}Tam cevap: '{correct_answer}'",
        ],
        "YanlÄ±ÅŸ": [
            f"âŒ {topic_text}YanlÄ±ÅŸ. {conf_text}DoÄŸru cevap: '{correct_answer}'",
            f"ğŸ”´ {topic_text}Maalesef hatalÄ±. {conf_text}DoÄŸrusu: '{correct_answer}'",
        ]
    }
    
    return random.choice(messages.get(label, messages["YanlÄ±ÅŸ"]))

def get_consensus(mbart_result: dict, mt5_result: dict) -> dict:
    """Ä°ki modelin consensus sonucu - KatÄ± mod"""
    scores = {'Tam DoÄŸru': 3, 'Ã‡ok Benzer': 2, 'KÄ±smen DoÄŸru': 1, 'YanlÄ±ÅŸ': 0}
    
    if not mbart_result and not mt5_result:
        return {"label": "YanlÄ±ÅŸ", "confidence": 0}
    if not mbart_result:
        return {"label": mt5_result['label'], "confidence": mt5_result['confidence'] / 100}
    if not mt5_result:
        return {"label": mbart_result['label'], "confidence": mbart_result['confidence'] / 100}
    
    # KatÄ± mod: Sadece aynÄ± etiketlerde consensus
    if STRICT_MODE:
        if mbart_result['label'] == mt5_result['label']:
            # AynÄ± etiket - gÃ¼ven skorunu artÄ±r
            confidence = min(95, (mbart_result['confidence'] + mt5_result['confidence']) / 2 + 10)
            return {"label": mbart_result['label'], "confidence": confidence / 100}
        else:
            # FarklÄ± etiketler - daha katÄ± kurallar
            mbart_score = scores[mbart_result['label']]
            mt5_score = scores[mt5_result['label']]
            
            # BÃ¼yÃ¼k fark varsa yanlÄ±ÅŸ yap
            if abs(mbart_score - mt5_score) >= 2:
                return {"label": "YanlÄ±ÅŸ", "confidence": 0.3}
            
            # KÃ¼Ã§Ã¼k fark - daha dÃ¼ÅŸÃ¼k skor al
            final_score = min(mbart_score, mt5_score)
            for label, score in scores.items():
                if score == final_score:
                    return {"label": label, "confidence": 0.6}
    else:
        # Normal mod - orijinal algoritma
        avg_score = (scores[mbart_result['label']] + scores[mt5_result['label']]) / 2
        
        final_label = "YanlÄ±ÅŸ"
        for label, score in scores.items():
            if abs(score - avg_score) < 0.6:
                final_label = label
                break
        
        confidence = (mbart_result['confidence'] + mt5_result['confidence']) / 200
        return {"label": final_label, "confidence": confidence}

def analyze_answer(question: str, student_answer: str, correct_answer: str, 
                   confidence: int = 50, topic: str = "", strict_mode: bool = STRICT_MODE, 
                   similarity_threshold: float = SIMILARITY_THRESHOLD) -> tuple:
    """Ana analiz fonksiyonu"""
    try:
        # Validasyon
        if not question or not student_answer or not correct_answer:
            return "âŒ Hata", "LÃ¼tfen tÃ¼m alanlarÄ± doldurun!", "{}", 0.0
        
        logger.info(f"ğŸ“ Analiz baÅŸlÄ±yor: {question[:30]}...")
        logger.info(f"ğŸ”§ Ayarlar - KatÄ± Mod: {strict_mode}, Benzerlik EÅŸiÄŸi: {similarity_threshold}")
        
        # Global ayarlarÄ± gÃ¼ncelle
        global STRICT_MODE, SIMILARITY_THRESHOLD
        STRICT_MODE = strict_mode
        SIMILARITY_THRESHOLD = similarity_threshold
        
        # Tahminler
        mbart_result = predict_mbart(question, student_answer, correct_answer)
        mt5_result = predict_mt5(question, student_answer, correct_answer)
        
        # Consensus
        consensus = get_consensus(mbart_result, mt5_result)
        
        # Feedback
        feedback = create_feedback(
            consensus['label'], student_answer, correct_answer, 
            confidence, topic
        )
        
        # Detaylar
        details = {
            "mBART": mbart_result['label'] if mbart_result else "N/A",
            "MT5": mt5_result['label'] if mt5_result else "N/A",
            "Consensus": consensus['label'],
            "Device": device
        }
        
        logger.info(f"âœ… SonuÃ§: {consensus['label']}")
        
        return (
            consensus['label'],
            feedback,
            json.dumps(details, ensure_ascii=False, indent=2),
            round(consensus['confidence'] * 100, 1)
        )
        
    except Exception as e:
        logger.error(f"âŒ Analiz hatasÄ±: {e}")
        return "âŒ Hata", f"Bir hata oluÅŸtu: {str(e)}", "{}", 0.0

# BaÅŸlangÄ±Ã§ta modelleri yÃ¼kle
logger.info("â³ Modeller yÃ¼kleniyor, lÃ¼tfen bekleyin...")
load_models()

# Gradio arayÃ¼zÃ¼
with gr.Blocks(
    title="MetaMind NLP - Ã–ÄŸrenci Cevap Analizi",
    theme=gr.themes.Soft(primary_hue="blue", secondary_hue="purple")
) as demo:
    
    gr.Markdown("# ğŸ§  MetaMind NLP - Ã–ÄŸrenci Cevap Analizi")
    gr.Markdown("TÃ¼rkÃ§e Ã¶ÄŸrenci cevaplarÄ±nÄ± analiz eder ve kiÅŸiselleÅŸtirilmiÅŸ feedback saÄŸlar.")
    
    with gr.Row():
        with gr.Column(scale=1):
            gr.Markdown("### ğŸ“ GiriÅŸ")
            
            question = gr.Textbox(
                label="Soru",
                placeholder="Ã–rn: DÃ¼nyanÄ±n en kalabalÄ±k Ã¼lkesi hangisidir?",
                lines=3
            )
            
            student_answer = gr.Textbox(
                label="Ã–ÄŸrenci CevabÄ±",
                placeholder="Ã–rn: Ã§in"
            )
            
            correct_answer = gr.Textbox(
                label="DoÄŸru Cevap",
                placeholder="Ã–rn: Ã§in"
            )
            
            confidence_slider = gr.Slider(
                label="Ã–ÄŸrenci GÃ¼ven Skoru (%)",
                minimum=0, maximum=100, value=50, step=1
            )
            
            topic = gr.Textbox(
                label="Konu (Opsiyonel)",
                placeholder="Ã–rn: DÃ¼nya CoÄŸrafyasÄ±"
            )
            
            # Model ayarlarÄ±
            with gr.Row():
                strict_mode = gr.Checkbox(
                    label="ğŸ”’ KatÄ± Mod (Daha Az Tolerans)",
                    value=STRICT_MODE,
                    info="AÃ§Ä±k: Daha katÄ± deÄŸerlendirme, KapalÄ±: Daha esnek"
                )
                similarity_threshold = gr.Slider(
                    label="Benzerlik EÅŸiÄŸi",
                    minimum=0.0, maximum=1.0, value=SIMILARITY_THRESHOLD, step=0.1,
                    info="YÃ¼ksek deÄŸer = Daha katÄ± benzerlik kontrolÃ¼"
                )
            
            analyze_btn = gr.Button("ğŸ” Analiz Et", variant="primary", size="lg")
        
        with gr.Column(scale=1):
            gr.Markdown("### ğŸ“Š SonuÃ§")
            
            result_label = gr.Textbox(label="ğŸ·ï¸ DeÄŸerlendirme", interactive=False)
            result_feedback = gr.Textbox(label="ğŸ’¬ Geri Bildirim", lines=4, interactive=False)
            result_confidence = gr.Number(label="ğŸ“ˆ GÃ¼ven Skoru (%)", interactive=False)
            result_details = gr.Textbox(label="ğŸ” Detaylar", lines=6, interactive=False)
    
    gr.Markdown("---")
    gr.Markdown("### ğŸ’¡ Ã–rnek KullanÄ±mlar")
    
    gr.Examples(
        examples=[
            ["DÃ¼nyanÄ±n en kalabalÄ±k Ã¼lkesi hangisidir?", "Ã§in", "Ã§in", 85, "DÃ¼nya CoÄŸrafyasÄ±"],
            ["TÃ¼rkiye'nin baÅŸkenti neresidir?", "istanbul", "ankara", 60, "TÃ¼rkiye CoÄŸrafyasÄ±"],
            ["2 + 2 kaÃ§ eder?", "4", "4", 95, "Matematik"],
            ["GÃ¼neÅŸ sisteminin en bÃ¼yÃ¼k gezegeni?", "jÃ¼piter", "jÃ¼piter", 70, "Astronomi"],
            ["Mona Lisa'yÄ± kim Ã§izdi?", "leonardo", "leonardo da vinci", 50, "Sanat"],
        ],
        inputs=[question, student_answer, correct_answer, confidence_slider, topic],
    )
    
    gr.Markdown("""
    ---
    ### ğŸ“– DeÄŸerlendirme Kriterleri
    
    - **ğŸ¯ Tam DoÄŸru (3)**: Cevap tamamen doÄŸru
    - **ğŸ“Œ Ã‡ok Benzer (2)**: Cevap doÄŸruya Ã§ok yakÄ±n
    - **ğŸ“š KÄ±smen DoÄŸru (1)**: Cevap kÄ±smen doÄŸru
    - **âŒ YanlÄ±ÅŸ (0)**: Cevap yanlÄ±ÅŸ
    
    ### ğŸ¤– KullanÄ±lan Modeller
    
    - **mBART**: [Ozget/MetaMind_Nlp_MBART_2](https://huggingface.co/Ozget/MetaMind_Nlp_MBART_2)
    - **MT5**: [Ozget/MetaMind_Nlp_MT5_2](https://huggingface.co/Ozget/MetaMind_Nlp_MT5_2)
    
    Ä°ki model birlikte Ã§alÄ±ÅŸarak daha gÃ¼venilir sonuÃ§lar Ã¼retir.
    """)
    
    # Event handler
    analyze_btn.click(
        fn=analyze_answer,
        inputs=[question, student_answer, correct_answer, confidence_slider, topic, strict_mode, similarity_threshold],
        outputs=[result_label, result_feedback, result_details, result_confidence],
        api_name="predict"  # API endpoint iÃ§in
    )

# Launch
if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7860)

